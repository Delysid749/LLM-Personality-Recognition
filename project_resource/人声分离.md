

根据资料，以下是能够区分两人对话的音频模型及其对应的现成GitHub项目，按技术框架和性能特点分类整理：

---

### 一、分段-聚类模块化框架
#### 1. **CAM++ 和 ERes2Net-Large（阿里云 ModelScope）**
- **技术说明**：  
  基于分段-聚类四步流程（VAD去除非人声→音频切段→特征提取→聚类），支持多人对话场景的说话人日志（Speaker Diarization）。  
  - **CAM++**：DER（说话人区分错误率）为4.7% []。  
  - **ERes2Net-Large**：在2人对话测试集上错误率4.8% []。  
- **GitHub**：  
  - 集成于阿里ModelScope平台，代码可在[ModelScope GitHub](https://github.com/modelscope/modelscope)查看。  
  - 3D-Speaker项目提供扩展功能：[https://github.com/modelscope/3D-Speaker ](https://github.com/modelscope/3D-Speaker )[]。

---

### 二、端到端说话人分离模型
#### 2. **pyannote.audio**
- **技术说明**：  
  基于PyTorch的端到端说话人分割模型，支持实时处理（1小时音频约3分钟），输出RTTM格式结果。支持预设说话人数（如2人）[]。  
- **GitHub**：[https://github.com/pyannote/pyannote-audio ](https://github.com/pyannote/pyannote-audio )[]。  
- **衍生项目**：  
  - **diart**：实时音频处理框架，集成说话人分段和嵌入模型，支持Web部署 []。  
  - **pyannote-whisper**：结合Whisper的ASR和pyannote的说话人分割，适用于会议记录分析 []。

#### 3. **whisper-diarization**
- **技术说明**：  
  结合OpenAI Whisper的语音识别与说话人分离技术，输出带说话人标签的文本转录。适合多人对话场景，错误率显著低于传统方法 []。  
- **GitHub**：[https://github.com/MahmoudAshraf97/whisper-diarization ](https://github.com/MahmoudAshraf97/whisper-diarization )[]。

---

### 三、多模态说话人识别
#### 4. **3D-Speaker（阿里通义实验室）**
- **技术说明**：  
  结合声学、语义（BERT）和视觉信息的多模态模型，提升复杂环境下的说话人识别能力。支持说话人日志、语种识别和重叠检测 []。  
- **性能**：在分割错误率（DER）上较纯声学模型提升显著 []。  
- **GitHub**：[https://github.com/modelscope/3D-Speaker ](https://github.com/modelscope/3D-Speaker )[]。

---

### 四、工业级语音识别工具包集成
#### 5. **FunASR**
- **技术说明**：  
  提供语音识别、VAD、说话人分离（如cam++模型）等全流程功能。支持Paraformer-large非自回归模型，适合长音频和实时处理 []。  
- **性能**：在工业数据上预训练，支持多人对话识别和说话人标签输出 []。  
- **GitHub**：[https://github.com/alibaba/FunASR ](https://github.com/alibaba/FunASR )[]。

---

### 五、其他开源项目
#### 6. **VGG-Speaker-Recognition & Kersa-Speaker-Recognition**
- **技术说明**：基于VGG网络和声纹识别技术，适合区分固定说话人（需预注册声纹）[]。  
- **GitHub**：  
  - VGG-Speaker：[https://github.com/WeidiXie/VGG-Speaker-Recognition ](https://github.com/WeidiXie/VGG-Speaker-Recognition )[]。  
  - Kersa：[https://github.com/yeyupiaoling/Kersa-Speaker-Recognition ](https://github.com/yeyupiaoling/Kersa-Speaker-Recognition )[]。

#### 7. **TranscriptionStream**
- **技术说明**：离线转录服务，集成说话人分离和AI摘要生成，支持自托管部署 []。  
- **GitHub**：未明确提及，需在GitHub搜索项目名称。

#### 8. **WhisperX & faster-whisper**
- **技术说明**：  
  - **WhisperX**：基于Whisper-large-v2，支持单词级时间戳和说话人分离，转录速度达实时70倍 []。  
  - **faster-whisper**：优化版Whisper，内存占用低（<8GB GPU）[]。  
- **GitHub**：  
  - WhisperX：[https://github.com/m-bain/whisperX ](https://github.com/m-bain/whisperX )[]。  
  - faster-whisper：[https://github.com/guillaumekln/faster-whisper ](https://github.com/guillaumekln/faster-whisper )[]。

---

### 六、对比与选型建议

| 项目                | 技术特点                         | 适用场景               | 性能优势                 |
| ------------------- | -------------------------------- | ---------------------- | ------------------------ |
| pyannote.audio      | 端到端实时处理，支持预设说话人数 | 会议记录、实时转录     | 处理速度快，支持动态调整 |
| 3D-Speaker          | 多模态（声学+语义+视觉）         | 复杂声学环境、多人重叠 | DER低于5%                |
| FunASR              | 工业级全流程工具包               | 长音频、客服对话分析   | 支持中文优化，高精度     |
| whisper-diarization | 结合Whisper的ASR与说话人分离     | 学术研究、多语言场景   | 转录与说话人标签一体化   |

---

### 七、注意事项
1. **数据要求**：部分模型（如VGG-Speaker）需预注册说话人声纹，而分段-聚类模型（如CAM++）无需预训练数据 [[1]()]。  
2. **硬件需求**：GPU加速项目（如WhisperX）需至少8GB显存，而pyannote.audio支持CPU推理 [[7]()]。  
3. **多语言支持**：FunASR和Whisper系列支持多语言，3D-Speaker专注中文优化 [[12](), [40]()]。

以上项目均提供开源代码，用户可根据场景需求（实时性、精度、硬件条件）选择合适方案。