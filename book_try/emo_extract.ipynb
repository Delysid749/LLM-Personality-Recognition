{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:09:46.440421Z",
     "start_time": "2025-03-13T13:09:35.185129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install -U funasr\n",
    "'''\n",
    "Using the finetuned emotion recognization model\n",
    "\n",
    "rec_result contains {'feats', 'labels', 'scores'}\n",
    "\textract_embedding=False: 9-class emotions with scores\n",
    "\textract_embedding=True: 9-class emotions with scores, along with features\n",
    "\n",
    "9-class emotions:\n",
    "iic/emotion2vec_plus_seed, iic/emotion2vec_plus_base, iic/emotion2vec_plus_large (May. 2024 release)\n",
    "iic/emotion2vec_base_finetuned (Jan. 2024 release)\n",
    "    0: angry\n",
    "    1: disgusted\n",
    "    2: fearful\n",
    "    3: happy\n",
    "    4: neutral\n",
    "    5: other\n",
    "    6: sad\n",
    "    7: surprised\n",
    "    8: unknown\n",
    "'''\n",
    "\n",
    "from funasr import AutoModel\n",
    "\n",
    "# model=\"iic/emotion2vec_base\"\n",
    "# model=\"iic/emotion2vec_base_finetuned\"\n",
    "# model=\"iic/emotion2vec_plus_seed\"\n",
    "# model=\"iic/emotion2vec_plus_base\"\n",
    "model_id = \"iic/emotion2vec_plus_large\"\n",
    "\n",
    "model = AutoModel(\n",
    "    model=model_id,\n",
    "    hub=\"ms\",  # \"ms\" or \"modelscope\" for China mainland users; \"hf\" or \"huggingface\" for other overseas users\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "86249ff94b9c2e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\19389\\.cache\\modelscope\\hub\\models\\iic\\emotion2vec_plus_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 21:09:42,271 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect model requirements, begin to install it: C:\\Users\\19389\\.cache\\modelscope\\hub\\models\\iic\\emotion2vec_plus_large\\requirements.txt\n",
      "install model requirements successfully\n",
      "ckpt: C:\\Users\\19389\\.cache\\modelscope\\hub\\models\\iic\\emotion2vec_plus_large\\model.pt\n",
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:10:52.359002Z",
     "start_time": "2025-03-13T13:10:52.354833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# wav_file = f\"{model.model_path}/example/test.wav\"\n",
    "wav_file = \"../data/voice/1DCnIad1Y0w.002.wav\"\n"
   ],
   "id": "29c35bbd085ed34d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:10:56.432485Z",
     "start_time": "2025-03-13T13:10:54.163977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rec_result = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\n",
    "print(rec_result)"
   ],
   "id": "24880fae3c5a383d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rtf_avg: 0.148: 100%|\u001B[34m██████████\u001B[0m| 1/1 [00:02<00:00,  2.26s/it]                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': 'output', 'labels': ['生气/angry', '厌恶/disgusted', '恐惧/fearful', '开心/happy', '中立/neutral', '其他/other', '难过/sad', '吃惊/surprised', '<unk>'], 'scores': [3.12398998542681e-14, 2.9636185270177223e-14, 4.654729377683237e-15, 1.0, 2.887142902707315e-17, 3.583470821543844e-17, 3.3797961377832718e-15, 3.8882986457336455e-14, 1.3343034939560418e-24]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T13:10:58.910591Z",
     "start_time": "2025-03-13T13:10:58.905231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#找到字典中scorse最高对应的labels\n",
    "max_score = max(rec_result[0]['scores'])\n",
    "max_index = rec_result[0]['scores'].index(max_score)\n",
    "max_label = rec_result[0]['labels'][max_index]\n",
    "print(f\"The most possible emotion is {max_label} with score {max_score}\")"
   ],
   "id": "b6d3dc510ca0d0c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most possible emotion is 开心/happy with score 1.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T09:36:52.569577Z",
     "start_time": "2025-03-13T09:36:52.530116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from playsound import playsound\n",
    "playsound(wav_file)"
   ],
   "id": "441af2ccf4fea576",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:playsound:\n",
      "    Error 263 for command:\n",
      "        open ../data/wav/test.wav\n",
      "    指定的设备未打开，或不被 MCI 所识别。\n",
      "ERROR:playsound:\n",
      "    Error 263 for command:\n",
      "        close ../data/wav/test.wav\n",
      "    指定的设备未打开，或不被 MCI 所识别。\n",
      "WARNING:playsound:Failed to close the file: ../data/wav/test.wav\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 263 for command:\n        open ../data/wav/test.wav\n    指定的设备未打开，或不被 MCI 所识别。",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPlaysoundException\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mplaysound\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m playsound\n\u001B[1;32m----> 2\u001B[0m \u001B[43mplaysound\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwav_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\environment\\miniconda\\envs\\voice\\lib\\site-packages\\playsound.py:72\u001B[0m, in \u001B[0;36m_playsoundWin\u001B[1;34m(sound, block)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     71\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m     \u001B[43mwinCommand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mopen \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43msound\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m     winCommand(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplay \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(sound, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m wait\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m block \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     74\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReturning\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mE:\\environment\\miniconda\\envs\\voice\\lib\\site-packages\\playsound.py:64\u001B[0m, in \u001B[0;36m_playsoundWin.<locals>.winCommand\u001B[1;34m(*command)\u001B[0m\n\u001B[0;32m     60\u001B[0m     exceptionMessage \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    Error \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(errorCode) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for command:\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     61\u001B[0m                         \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m command\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-16\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m     62\u001B[0m                         \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m errorBuffer\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-16\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\0\u001B[39;00m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     63\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(exceptionMessage)\n\u001B[1;32m---> 64\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PlaysoundException(exceptionMessage)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m buf\u001B[38;5;241m.\u001B[39mvalue\n",
      "\u001B[1;31mPlaysoundException\u001B[0m: \n    Error 263 for command:\n        open ../data/wav/test.wav\n    指定的设备未打开，或不被 MCI 所识别。"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 看看两个音频文件有什么不同",
   "id": "dcdbbd379040a94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T09:26:46.126905Z",
     "start_time": "2025-03-13T09:26:45.292079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "\n",
    "def get_audio_properties(file_path):\n",
    "    # 加载音频文件\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    # 获取音频时长\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    # 获取通道数，librosa默认读取为单声道，若要获取原始通道数，需要额外处理\n",
    "    channels = 1 if len(y.shape) == 1 else y.shape[1]\n",
    "    return {'sample_rate': sr, 'channels': channels, 'duration': duration}\n",
    "\n",
    "def compare_audio_files(file1, file2):\n",
    "    props1 = get_audio_properties(file1)\n",
    "    props2 = get_audio_properties(file2)\n",
    "\n",
    "    print(\"File 1 properties:\", props1)\n",
    "    print(\"File 2 properties:\", props2)\n",
    "\n",
    "    # 比较属性\n",
    "    for key in props1.keys():\n",
    "        if props1[key] != props2[key]:\n",
    "            print(f\"Difference found in {key}: File 1 has {props1[key]}, File 2 has {props2[key]}\")\n",
    "        else:\n",
    "            print(f\"{key.capitalize()} is the same.\")\n",
    "\n",
    "# 替换成你的音频文件路径\n",
    "file1 = \"../data/wav/test.wav\"\n",
    "file2 = f\"{model.model_path}/example/test.wav\"\n",
    "compare_audio_files(file1, file2)"
   ],
   "id": "e63a4e7a75d760e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 properties: {'sample_rate': 48000, 'channels': 1, 'duration': 3.9372708333333333}\n",
      "File 2 properties: {'sample_rate': 16000, 'channels': 1, 'duration': 4.104125}\n",
      "Difference found in sample_rate: File 1 has 48000, File 2 has 16000\n",
      "Channels is the same.\n",
      "Difference found in duration: File 1 has 3.9372708333333333, File 2 has 4.104125\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
