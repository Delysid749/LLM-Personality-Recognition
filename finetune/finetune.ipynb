{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:55:55.142117Z",
     "start_time": "2025-03-20T09:55:28.754164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "id": "c545bfe1adb4c998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19389\\.conda\\envs\\finetune\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3725e52c0264c7e8c9b1db60f3c974c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:55:57.838097Z",
     "start_time": "2025-03-20T09:55:55.156948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "id": "ee40e24d1935d177",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.14 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:55:59.012142Z",
     "start_time": "2025-03-20T09:55:58.137085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "alpaca_prompt = \"\"\"\n",
    "### Instruction:\n",
    "# è§’è‰²\n",
    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å­¦å®¶ï¼Œæ“…é•¿é€šè¿‡åˆ†æä¸ªäººçš„è¡Œä¸ºå’Œè¨€è¯­æ¥è¯„ä¼°å…¶æ€§æ ¼ç‰¹è´¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯ï¼Œå¯¹ä¸ªä½“çš„æ€§æ ¼è¿›è¡Œè¯¦ç»†åˆ†æï¼Œå¹¶ç»™å‡ºäº”å¤§æ€§æ ¼ç‰¹è´¨çš„å…·ä½“è¯„åˆ†ã€‚\n",
    "\n",
    "## æŠ€èƒ½\n",
    "### æŠ€èƒ½1ï¼šæ€§æ ¼è¯„ä¼°\n",
    "- **ä»»åŠ¡**ï¼šåŸºäºç”¨æˆ·æä¾›çš„ä¿¡æ¯ï¼ˆå¦‚è¡Œä¸ºæè¿°ã€è¨€è¯­è¡¨è¾¾ç­‰ï¼‰ï¼Œå¯¹ä¸ªä½“çš„äº”å¤§æ€§æ ¼ç‰¹è´¨è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»™å‡ºä»0-1çš„å…·ä½“åˆ†æ•°ã€‚\n",
    "  - **å¼€æ”¾æ€§ï¼ˆOpennessï¼‰**ï¼šè¯„ä¼°ä¸ªä½“çš„å¥½å¥‡å¿ƒã€æƒ³è±¡åŠ›å’Œå¯¹æ–°äº‹ç‰©çš„æ¥å—ç¨‹åº¦ã€‚\n",
    "  - **è´£ä»»å¿ƒï¼ˆConscientiousnessï¼‰**ï¼šè¯„ä¼°ä¸ªä½“çš„è´£ä»»æ„Ÿã€ç»„ç»‡èƒ½åŠ›å’Œè‡ªå¾‹æ€§ã€‚\n",
    "  - **å¤–å‘æ€§ï¼ˆExtraversionï¼‰**ï¼šè¯„ä¼°ä¸ªä½“çš„ç¤¾äº¤èƒ½åŠ›ã€æ´»åŠ›å’Œä¹è§‚ç¨‹åº¦ã€‚\n",
    "  - **å®œäººæ€§ï¼ˆAgreeablenessï¼‰**ï¼šè¯„ä¼°ä¸ªä½“çš„åˆä½œæ€§ã€åŒæƒ…å¿ƒå’Œä¿¡ä»»åº¦ã€‚\n",
    "  - **ç¥ç»è´¨ï¼ˆNeuroticismï¼‰**ï¼šè¯„ä¼°ä¸ªä½“çš„æƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘æ°´å¹³å’Œå‹åŠ›åº”å¯¹èƒ½åŠ›ã€‚\n",
    "\n",
    "## é™åˆ¶\n",
    "- ä»…åŸºäºç”¨æˆ·æä¾›çš„ä¿¡æ¯è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿è¯„ä¼°ç»“æœå®¢è§‚å‡†ç¡®ã€‚\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "def formatting_prompts_func(examples):\n",
    "    # instructions = examples[\"introduction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for input_data, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input_data, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset,DatasetDict\n",
    "dataset = load_dataset(\"json\", data_files=\"../data/results.jsonl\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "full_dataset = dataset[\"train\"]  # æå–å®é™…æ•°æ®éƒ¨åˆ†\n",
    "# ç›´æ¥åˆ’åˆ†ï¼š90% è®­ç»ƒï¼Œ10% æµ‹è¯•\n",
    "train_test = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "# ç»„åˆä¸º DatasetDict\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": train_test[\"train\"],\n",
    "    \"test\": train_test[\"test\"],\n",
    "})\n",
    "# æŸ¥çœ‹åˆ’åˆ†ç»“æœ\n",
    "print(final_dataset)"
   ],
   "id": "e189ff99fca5b210",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97a57f8ab34b4370bd04dd484f8180b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88a7d67ff8bb4568a657ae8a76b155a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['introduction', 'input', 'output', 'text'],\n",
      "        num_rows: 72\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['introduction', 'input', 'output', 'text'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:56:28.619522Z",
     "start_time": "2025-03-20T09:55:59.020988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prrompt = f\"{final_dataset['test'][0]['introduction']}\\n{final_dataset['test'][0]['input']}\"\n",
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}ç»™å‡ºä»0-1çš„å…·ä½“åˆ†æ•°ã€‚\n",
    "### Input:\n",
    "{}\n",
    "è¯·ç›´æ¥ç»™å‡ºbig fiveçš„äº”ä¸ªç»´åº¦çš„å…·ä½“åˆ†æ•°ã€‚\n",
    "### Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt.format(final_dataset['test'][0]['introduction'], final_dataset['test'][0]['input'])\n",
    "prompt_encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **prompt_encoding,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "# å°†ç”Ÿæˆçš„è¾“å‡ºè§£ç ä¸ºæ–‡æœ¬\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(generated_text[0])"
   ],
   "id": "278b6ce25cae546e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "taçš„äº”å¤§æ€§æ ¼ä¸ºï¼Ÿç»™å‡ºä»0-1çš„å…·ä½“åˆ†æ•°ã€‚\n",
      "### Input:\n",
      "From the audio analysis, the speaker said: fall asleep and I'm not going to be tired tired.My favorite food was it is still chocolate Cholate is the world's greatest food, it is the most incredible food I never thought there would be life without chocolate, there is life, there is life without chocolate, lamb chops..\n",
      "The most possible emotion is å¼€å¿ƒ/happy with score 0.9999736547470093. \n",
      "His speech rate is 3.1368550834597877 words per second, the average volume is -13.48 dB \t the standard deviation of the volume is 6.12 dB. The average pitch is 205.05 Hz \t the standard deviation of the pitch is:38.93 Hz\n",
      "è¯·ç›´æ¥ç»™å‡ºbig fiveçš„äº”ä¸ªç»´åº¦çš„å…·ä½“åˆ†æ•°ã€‚\n",
      "### Response:\n",
      "\n",
      "Assistant: æ ¹æ®éŸ³é¢‘åˆ†æï¼Œæ¼”è®²è€…è¯´ï¼šâ€œfall asleep and I'm not going to be tired tired.My favorite food was it is still chocolate Cholate is the world's greatest food, it is the most incredible food I never thought there would be life without chocolate, there is life, there is life without chocolate, lamb chops..â€ \n",
      "ä»éŸ³é¢‘ä¸­å¯ä»¥å¾—å‡ºï¼Œæ¼”è®²è€…çš„æƒ…ç»ªæ˜¯å¼€å¿ƒ/happyï¼Œå¾—åˆ†æ˜¯0.9999736547470093ã€‚ \n",
      "ä»–çš„è¯­é€Ÿæ˜¯3.136855083459787\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:56:29.010696Z",
     "start_time": "2025-03-20T09:56:28.667458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = final_dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 1,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ],
   "id": "1de90d0f59f3aa52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f97a726ce05487eae857e698712a08d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:56:29.073257Z",
     "start_time": "2025-03-20T09:56:29.059178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ],
   "id": "2c27a0c58dee5148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4060 Laptop GPU. Max memory = 7.996 GB.\n",
      "7.672 GB of memory reserved.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-20T09:56:29.120433Z"
    }
   },
   "cell_type": "code",
   "source": "trainer_stats = trainer.train()",
   "id": "6ebf016d37aedbc1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 72 | Num Epochs = 7 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176/7,000,000,000 (0.58% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/60 09:26 < 2:53:05, 0.01 it/s, Epoch 0.44/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.862500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:55:42.270758Z",
     "start_time": "2025-03-20T08:55:42.257630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ],
   "id": "4802dbcee5945ae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7434.3998 seconds used for training.\n",
      "123.91 minutes used for training.\n",
      "Peak reserved memory = 8.889 GB.\n",
      "Peak reserved memory for training = 1.217 GB.\n",
      "Peak reserved memory % of max memory = 111.168 %.\n",
      "Peak reserved memory for training % of max memory = 15.22 %.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:55:42.332979Z",
     "start_time": "2025-03-20T08:55:42.318493Z"
    }
   },
   "cell_type": "code",
   "source": "final_dataset[\"test\"]",
   "id": "4bd7c071b10c481b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['introduction', 'input', 'output', 'text'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:56:05.771232Z",
     "start_time": "2025-03-20T08:55:42.444235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prrompt = f\"{final_dataset['test'][0]['introduction']}\\n{final_dataset['test'][0]['input']}\"\n",
    "prompt = \"\"\"\n",
    "### Instruction:\n",
    "{}ç»™å‡ºä»0-1çš„å…·ä½“åˆ†æ•°ã€‚\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt.format(final_dataset['test'][0]['introduction'], final_dataset['test'][0]['input'])\n",
    "prompt_encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **prompt_encoding,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "# å°†ç”Ÿæˆçš„è¾“å‡ºè§£ç ä¸ºæ–‡æœ¬\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(generated_text[0])"
   ],
   "id": "baa09d75cb168fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "taçš„äº”å¤§æ€§æ ¼ä¸ºï¼Ÿç»™å‡ºä»0-1çš„å…·ä½“åˆ†æ•°ã€‚\n",
      "### Input:\n",
      "From the audio analysis, the speaker said: fall asleep and I'm not going to be tired tired.My favorite food was it is still chocolate Cholate is the world's greatest food, it is the most incredible food I never thought there would be life without chocolate, there is life, there is life without chocolate, lamb chops..The most possible emotion is å¼€å¿ƒ/happy with score 0.9999736547470093. His speech rate is 3.1368550834597877 words per second, the average volume is -13.48 dB \t the standard deviation of the volume is 6.12 dB. The average pitch is 205.05 Hz \t the standard deviation of the pitch is:38.93 Hz\n",
      "### Response:\n",
      "\n",
      "taçš„äº”å¤§æ€§æ ¼ä¸ºï¼š\n",
      "å¤–å‘æ€§ï¼š0 score 0 score 0 score 0 score 0 score\n",
      "å†…å‘æ€§ï¼š0 score 0 score 0 score 0 score 0 score\n",
      "å¤–å‘æ€§ï¼š0 score 0 score 0 score 0 score 0 score\n",
      "å¤–å‘æ€§ï¼š0.688<0.7ï¼Œå¤–å‘æ€§ä¸€èˆ¬ã€‚\n",
      "å¤–å‘æ€§ï¼š0.688<0.7ï¼Œå¤–å‘æ€§ä¸€èˆ¬ã€‚\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:56:05.818284Z",
     "start_time": "2025-03-20T08:56:05.803468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ],
   "id": "f1788fd899989a5b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
